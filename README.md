# SWE-Bench Analysis

This repository contains tools and scripts to evaluate AI models against SWE-Bench datasets, which assess the ability of AI models to suggest correct patches for software engineering tasks.

## Features

- **Experiment Results Processing:** Load and process experiment results from various models.
- **Benchmark Analysis:** Compare model predictions with ground truth across Lite, Full, and Verified SWE-Bench datasets.
- **Statistics Generation:** Compute detailed statistics, including resolved vs. unresolved instances and per-model performance.

---

## Dataset Structure

Three main benchmarks are evaluated:

1. **Lite Bench:** Smaller, more targeted dataset.

2. **Full Bench:** Complete dataset.

3. **Verified Bench:** Verified results from a wide range of experiments.
